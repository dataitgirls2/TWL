# 20180730 기계학습&Kaggle

### 오전 : 기계학습

- 국민청원 카테고리 예측력 높여보기

- B조 18% 예측력이 나옴

  - 데이터 범위 투표 100건 초과 1000건 미만
  - ngram을 4까지 올렸고
  - max_feature = 4000
  - estimator = 500
  - min_df = 4

  stopword 추가 했을 때 18% 예측력이 나왔음

-  벡터화 

  - analyzer = 'character'넣었을 경우, ngram을 높게 지정하는 편이 좋음
  - tokenizer = None # 토크나이저를 따로 지정해 줄 수도 있음
  - preprocessor = None # 전처리 도구
  - stop_words = None # 불용어 nltk등의 도구를 사용할 수도 있음
  - min_df = 2 # 토큰이 나타날 최소 문서 개수로 오타나 자주 나오지 않는 특수한 전문용어 제거에 좋음
  -  ngram_range=(1, 4) # BOW의 단위를 1~3개로 지정
  - max_features = 4000 # 만들 피처의 수, 단어의 수

- 랜덤포레스트 

  - n_estimators가 가지는 의미?
  - n_jobs = -1 나의 cpu를 모두 사용한다는 의미

- 기타분야의 데이터를 제거하거나, 만든 셋으로 기타데이터를 다른 분야로 재배치하는 방법도 있었음



### 오후 : [Kaggle NLP](https://www.kaggle.com/c/word2vec-nlp-tutorial)

- 캐글점수 0.85420 (edited) / 54% / 312등
  - min_df =3
  - ngram = 4 했을때

- part2 단어의 모수가 더 높게 만들면 높은 점수를 얻을 수 있음
- part3 비슷한 단어의 클러스트링으로 단어의 벡터를 지정함

[MLWave](https://mlwave.com/online-learning-perceptron/) 를 사용하니 높은 예측률을 보였음



### 과제 GitHub 저장소 최신상태로 유지하기
